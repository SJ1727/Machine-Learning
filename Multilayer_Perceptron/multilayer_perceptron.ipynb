{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import HMDL.Tensors as ht\n",
    "from HMDL.nn import ReLUMultilayerPerceptron, CrossEntropyLoss\n",
    "from HMDL.optim import StochasticGradientDescent, Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "You can download the mnist dataset from [here](https://www.kaggle.com/datasets/oddrationale/mnist-in-csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/mnist_train.csv\")  \n",
    "test_df = pd.read_csv(\"../data/mnist_test.csv\")[:1000]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our model and hyperparameters\n",
    "\n",
    "Since we are doing a classification problem we will be using the cross entropy loss. If we were doing linear regression we may use something like MSEloss.\n",
    "\n",
    "The optimizer we will be using is the adam optimizer which is one of the most commonly used and most performant. The formula for the adam optimizer is as follows:\n",
    "\n",
    "$$m_t=\\beta_1 m_{t-1} + (1 - \\beta_1) * \\frac{\\partial L}{\\partial w}\\alpha$$\n",
    "$$v_t=\\beta_2 v_{t-1} + (1 - \\beta_2) * (\\frac{\\partial L}{\\partial w}\\alpha)^2$$\n",
    "$$\\hat m_t=\\frac{m_t}{1 - \\beta_1^t}$$\n",
    "$$\\hat v_t=\\frac{v_t}{1 - \\beta_2^t}$$\n",
    "\n",
    "$$w_{t}=w_{t-1}-\\frac{\\hat m_t}{\\sqrt{\\hat v_t} + \\epsilon}\\gamma$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 1\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "net = ReLUMultilayerPerceptron([784, 32, 10])\n",
    "optim = Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and formating out data\n",
    "\n",
    "Our image data will be in the shape $(B\\times C \\times HW)$ were $B$ is our batch size, $C$ is the number of color channels (1 in our case) and $H, W$ are the height and width of our image (our image is 28 by 28 so $HW$ will be 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.eye(10)[train_df[\"label\"].to_numpy()]\n",
    "train_labels = [\n",
    "    ht.Tensor(\n",
    "        np.array(train_labels[i:i + BATCH_SIZE][:, np.newaxis, :])\n",
    "    )\n",
    "    for i in range(0, len(train_labels), BATCH_SIZE)\n",
    "]\n",
    "\n",
    "train_data = train_df.drop([\"label\"], axis=1).to_numpy() / 255\n",
    "train_data = [\n",
    "    ht.Tensor(\n",
    "        np.array(train_data[i:i + BATCH_SIZE][:, np.newaxis, :])\n",
    "    )\n",
    "    for i in range(0, len(train_data), BATCH_SIZE)\n",
    "]\n",
    "\n",
    "test_labels = np.eye(10)[test_df[\"label\"].to_numpy()]\n",
    "test_labels = [\n",
    "    ht.Tensor(\n",
    "        np.array(test_labels[i:i + TEST_SIZE][:, np.newaxis, :])\n",
    "    )\n",
    "    for i in range(0, len(test_labels), TEST_SIZE)\n",
    "]\n",
    "\n",
    "test_data = test_df.drop([\"label\"], axis=1).to_numpy() / 255\n",
    "test_data = [\n",
    "    ht.Tensor(\n",
    "        np.array(test_data[i:i + TEST_SIZE][:, np.newaxis, :])\n",
    "    )\n",
    "    for i in range(0, len(test_data), TEST_SIZE)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data, test_labels):\n",
    "    correct = 0\n",
    "\n",
    "    # Counts the number of times the network gets the correct result\n",
    "    for label, image in zip(test_labels, test_data):\n",
    "        result = model.forward(image)\n",
    "        prediciton = np.argmax(result.data, axis=-1)\n",
    "        correct += np.count_nonzero(prediciton == np.argmax(label.data, axis=-1))\n",
    "\n",
    "    print(f\"Accuracy: {correct / len(test_data) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model\n",
    "\n",
    "After every training epoch in our training loop we will test the performance of our model by testing its accuracy on our test dataset. Our test dataset does not contain any of the examples which are in our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for label, image in zip(train_labels, train_data):\n",
    "        # Get the predicted results from the network\n",
    "        result = net.forward(image)\n",
    "        \n",
    "        # Zero all the gradients of the parameters\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(result, label)\n",
    "        \n",
    "        # Backpropogate through the network\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the networks parameters\n",
    "        optim.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Loss: {-1 / 10 * loss.data}\")\n",
    "    test_model(net, test_data, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
