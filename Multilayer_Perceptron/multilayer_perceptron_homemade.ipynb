{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import homemade_tensors as ht\n",
    "from homemade_nn import MultilayerPerceptron, StochasticGradientDescent, CrossEntropyLoss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "You can download the mnist dataset from [here](https://www.kaggle.com/datasets/oddrationale/mnist-in-csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/mnist_train.csv\")  \n",
    "test_df = pd.read_csv(\"../data/mnist_test.csv\")[:1000]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our model and hyperparameters\n",
    "\n",
    "Since we are doing a classification problem we will be using the cross entropy loss. If we were doing linear regression we may use something like MSEloss.\n",
    "\n",
    "The optimizer we will be using is stochastic gradient descent which is the most basic kind which use the formula below to update the parameters.\n",
    "\n",
    "$$w_{t+1}=w_{t}-\\frac{\\partial L}{\\partial w}\\alpha$$\n",
    "\n",
    "A more popular optimizer we could use is the adam optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 1\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "net = MultilayerPerceptron()\n",
    "optim = StochasticGradientDescent(net.parameters(), lr=LEARNING_RATE)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and formating out data\n",
    "\n",
    "Our image data will be in the shape $(B\\times C \\times HW)$ were $B$ is our batch size, $C$ is the number of color channels (1 in our case) and $H, W$ are the height and width of our image (our image is 28 by 28 so $HW$ will be 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.eye(10)[train_df[\"label\"].to_numpy()]\n",
    "train_labels = [\n",
    "    ht.Tensor(\n",
    "        np.array(train_labels[i:i + BATCH_SIZE][:, np.newaxis, :])\n",
    "    )\n",
    "    for i in range(0, len(train_labels), BATCH_SIZE)\n",
    "]\n",
    "\n",
    "train_data = train_df.drop([\"label\"], axis=1).to_numpy() / 255\n",
    "train_data = [\n",
    "    ht.Tensor(\n",
    "        np.array(train_data[i:i + BATCH_SIZE][:, np.newaxis, :])\n",
    "    )\n",
    "    for i in range(0, len(train_data), BATCH_SIZE)\n",
    "]\n",
    "\n",
    "test_labels = np.eye(10)[test_df[\"label\"].to_numpy()]\n",
    "test_labels = [\n",
    "    ht.Tensor(\n",
    "        np.array(test_labels[i:i + TEST_SIZE][:, np.newaxis, :])\n",
    "    )\n",
    "    for i in range(0, len(test_labels), TEST_SIZE)\n",
    "]\n",
    "\n",
    "test_data = test_df.drop([\"label\"], axis=1).to_numpy() / 255\n",
    "test_data = [\n",
    "    ht.Tensor(\n",
    "        np.array(test_data[i:i + TEST_SIZE][:, np.newaxis, :])\n",
    "    )\n",
    "    for i in range(0, len(test_data), TEST_SIZE)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for label, image in zip(train_labels, train_data):\n",
    "        # Get the predicted results from the network\n",
    "        result = net.forward(image)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(result, label)\n",
    "        \n",
    "        # Backpropogate through the network\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the networks parameters\n",
    "        optim.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Loss: {-1 / 10 * loss.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our model\n",
    "\n",
    "We use a different dataset to test the accuracy of our model, this is ensure that our model has not over fit to our original training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "\n",
    "# Counts the number of times the network gets the correct result\n",
    "for label, image in zip(test_labels, test_data):\n",
    "    result = net.forward(image)\n",
    "    prediciton = np.argmax(result.data, axis=-1)\n",
    "    correct += np.count_nonzero(prediciton == np.argmax(label.data, axis=-1))\n",
    "\n",
    "print(f\"Accuracy: {correct / len(test_data) * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
